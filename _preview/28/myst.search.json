{"version":"1","records":[{"hierarchy":{"lvl1":"Landsat ML Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Landsat ML Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Landsat ML Cookbook"},"type":"lvl1","url":"/#landsat-ml-cookbook","position":2},{"hierarchy":{"lvl1":"Landsat ML Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers the essential materials for working with Landsat data in the context of machine learning workflows.","type":"content","url":"/#landsat-ml-cookbook","position":3},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Motivation"},"content":"Once you complete this cookbook, you will have the skills to access, resample, regrid, reshape, and rescale satellite data, as well as the foundation for applying machine learning to it. You will also learn how to interactively visualize your data at every step in the process.","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Authors"},"content":"Demetris Roumis\n\n\nAndrew Huang","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"\n\nThis cookbook was initially inspired by the \n\nEarthML . See a list of the EarthML contributors \n\nhere:\n\n\n","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into two main sections - “Foundations” and “Example Workflows.”","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Foundations","lvl2":"Structure"},"type":"lvl3","url":"/#foundations","position":12},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Foundations","lvl2":"Structure"},"content":"The foundational content includes:\n\nStart Here - Introduction to Landsat data.\n\nData Ingestion - Geospatial-Specific Tooling - Demonstrating a method for loading and accessing Landsat data from Microsoft’s Planetary Computer platform with tooling from pystac and odc.\n\nData Ingestion - General Purpose Tooling - Demonstrating approaches for domain-independent data access using Intake.","type":"content","url":"/#foundations","position":13},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"type":"lvl3","url":"/#example-workflows","position":14},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Example Workflows","lvl2":"Structure"},"content":"Example workflows include:\n\nSpectral Clustering - Demonstrating a machine learning approach to cluster pixels of satellite data and comparing cluster results across time","type":"content","url":"/#example-workflows","position":15},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":16},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":17},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":18},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":19},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":20},{"hierarchy":{"lvl1":"Landsat ML Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the Landsat ML Cookbook repository: git clone https://github.com/ProjectPythia/landsat-ml-cookbook.git\n\nMove into the landsat-ml-cookbook directorycd landsat-ml-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate landsat-ml-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":21},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data"},"type":"lvl1","url":"/notebooks/intro-landsat","position":0},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data"},"content":"\n\n\n\n","type":"content","url":"/notebooks/intro-landsat","position":1},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/intro-landsat#overview","position":2},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Overview"},"content":"In this cookbook, you will access, process, analyze, and visualize satellite data in the context of machine learning workflows. This particular cookbook notebook will provide an introduction to Landsat data to build our intuition as we move toward data ingestion, processing, and analysis.\n\nTime to learn: 5 minutes\n\n","type":"content","url":"/notebooks/intro-landsat#overview","position":3},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Landsat Data"},"type":"lvl2","url":"/notebooks/intro-landsat#landsat-data","position":4},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Landsat Data"},"content":"\n\nThe data in this cookbook originally come from the \n\nLandsat program, which is the longest record of moderate resolution multispectral data of the Earth’s surface. This program has launched several different satellites spanning many years which are designated as Landsat 1-9.\n\nWhen accessing the data, it’s important to keep in mind a couple key points. First, the instruments on different Landsat missions (1-9) varied in certain aspects. Second, Landsat data is available from multiple providers (USGS, NASA, Google, Microsoft, AWS, etc) but may vary in completeness and the level of processing applied. For the dataset that you end up using, it is crucial to review to relevant information from the particular data provider and the specific Landsat mission to understand the details, especially if you are comparing data across providers or missions.\n\nIn general, a common aspect of Landsat data is the use of different wavelength-bands to capture multiple images of the same area - together providing much more information about different features on the ground than a single image alone. This provides us with a stack of images for each spatial region that we might be interested.\n\nAdditionally, whenever we are looking at changes in satellite images over time, we will have an additional time dimension. For example, we will consider two stacks of images from different years to look at the change in the water level around a lake.\n\n\n\n","type":"content","url":"/notebooks/intro-landsat#landsat-data","position":5},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/intro-landsat#summary","position":6},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Summary"},"content":"Before accessing any data, it’s a good idea to start by learning about the context and details of the dataset. This will give you the intuition to make informed decisions as you form a processing and analysis pipeline.","type":"content","url":"/notebooks/intro-landsat#summary","position":7},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/intro-landsat#whats-next","position":8},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Next, we’ll learn about loading the data using the Microsoft Planetary Computer platform.\n\n","type":"content","url":"/notebooks/intro-landsat#whats-next","position":9},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/intro-landsat#resources-and-references","position":10},{"hierarchy":{"lvl1":"Start Here - Intro to Landsat Data","lvl2":"Resources and references"},"content":"The Landsat timeline image is originally from \n\nUSGS but discovered through \n\nearthsciencedata​.org\n\nThe Landsat 8 banner image is from NASA\n\nThe Landsat spectral bands is from \n\nNASA\n\nThis page was authored by Demetris Roumis circa Jan, 2023","type":"content","url":"/notebooks/intro-landsat#resources-and-references","position":11},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling"},"type":"lvl1","url":"/notebooks/data-ingestion-geospatial","position":0},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling"},"content":"\n\n\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial","position":1},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#overview","position":2},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Overview"},"content":"\n\nIn this notebook, you will ingest Landsat data for use in machine learning. Machine learning tasks often involve a lot of data, and in Python, data is typically stored in memory as simple \n\nNumPy arrays. However, higher-level containers built on top of NumPy arrays provide more functionality for multidimensional gridded data (\n\nxarray) or out-of-core and distributed data (\n\nDask). Our goal for data ingestion will be to load specific Landsat data of interest into one of these higher-level containers.\n\nMicrosoft Plantery Computer is one of several providers of \n\nLandsat Data. We are using it together with \n\npystac-client and \n\nodc-stac because together they provide a nice Python API for searching and loading with specific criteria such as spatial area, datetime, Landsat mission, and cloud coverage.\n\nEarth science datasets are often stored on remote servers that may be too large to download locally. Therefore, in this cookbook, we will focus primarily on ingestion approaches that load small portions of data from a remote source, as needed. However, the approach for your own work will depend not only on data size and location but also the intended analysis, so in a follow up notebook, you will see an alternative approache for generalized data access and management.\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#overview","position":3},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#prerequisites","position":4},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Landsat\n\nNecessary\n\nBackground\n\nAbout the Microsoft Planetary Computer\n\nHelpful\n\nBackground\n\npystac-client Usage\n\nHelpful\n\nConsult as needed\n\nodc.stac.load Reference\n\nHelpful\n\nConsult as needed\n\nxarray\n\nNecessary\n\n\n\nIntro to Dask Array\n\nHelpful\n\n\n\nPanel Getting Started Guide\n\nHelpful\n\n\n\nTime to learn: 10 minutes\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#prerequisites","position":5},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Imports"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#imports","position":6},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Imports"},"content":"\n\nimport odc.stac\nimport pandas as pd\nimport planetary_computer\nimport pystac_client\nimport xarray as xr\nfrom pystac.extensions.eo import EOExtension as eo\n\n# Viz\nimport hvplot.xarray\nimport panel as pn\n\npn.extension()\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#imports","position":7},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Open and read the root of the STAC catalog"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#open-and-read-the-root-of-the-stac-catalog","position":8},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Open and read the root of the STAC catalog"},"content":"\n\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\ncatalog.title\n\nMicrosoft Planetary Computer has a public STAC metadata but the actual data assets are in private Azure Blob Storage containers and require authentication. pystac-client provides a modifier keyword that we can use to manually sign the item. Otherwise, we’d get an error when trying to access the asset.\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#open-and-read-the-root-of-the-stac-catalog","position":9},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl2":"Search for Landsat Data"},"type":"lvl2","url":"/notebooks/data-ingestion-geospatial#search-for-landsat-data","position":10},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl2":"Search for Landsat Data"},"content":"\n\nLet’s say that an analysis we want to run requires landsat data over a specific region and from a specific time period. We can use our catalog to search for assets that fit our search criteria.\n\nFirst, let’s find the name of the landsat dataset. \n\nThis page is a nice resource for browsing the available collections, but we can also just search the catalog for ‘landsat’:\n\nall_collections = [i.id for i in catalog.get_collections()]\nlandsat_collections = [\n    collection for collection in all_collections if \"landsat\" in collection\n]\nlandsat_collections\n\nWe’ll use the landsat-c2-l2 dataset, which stands for Collection 2 Level-2. It contains data from several landsat missions and has better data quality than Level 1 (landsat-c2-l1). Microsoft Planetary Computer has descriptions of \n\nLevel 1 and \n\nLevel 2, but a direct and succinct comparison can be found in \n\nthis community post, and the information can be verified with \n\nUSGS.\n\nNow, let’s set our search parameters. You may already know the bounding box (region/area of interest) coordinates, but if you don’t, there are many useful tools like \n\nbboxfinder.com that can help.\n\nbbox = [-118.89, 38.54, -118.57, 38.84]  # Region over a lake in Nevada, USA\ndatetime = \"2017-06-01/2017-09-30\"  # Summer months of 2017\ncollection = \"landsat-c2-l2\"\n\nWe can also specify other parameters in the query, such as a specific landsat mission and the max percent of cloud cover:\n\nplatform = \"landsat-8\"\ncloudy_less_than = 1  # percent\n\nNow we run the search and list the results:\n\nsearch = catalog.search(\n    collections=[\"landsat-c2-l2\"],\n    bbox=bbox,\n    datetime=datetime,\n    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}, \"platform\": {\"in\": [platform]}},\n)\nitems = search.item_collection()\nprint(f\"Returned {len(items)} Items:\")\nitem_id = {(i, item.id): i for i, item in enumerate(items)}\nitem_id\n\nIt looks like there were three image stacks taken by Landsat 8 over this spatial region during the summer months of 2017 that has less than 1 percent cloud cover.\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#search-for-landsat-data","position":11},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Preview Results and Select a Dataset","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#preview-results-and-select-a-dataset","position":12},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Preview Results and Select a Dataset","lvl2":"Search for Landsat Data"},"content":"\n\nBefore loading one of the available image stacks, it would be useful to get a visual check of the results. Many datasets have a rendered preview or thumbnail image that can be accessed without having to load the full resolution data.\n\nWe can create a simple interactive application using the \n\nPanel library to access and display rendered PNG previews of the our search results. Note that these pre-rendered images are of large tiles that span beyond our bounding box of interest. In the next steps, we will only be loading in a small area around the lake.\n\nitem_sel = pn.widgets.Select(value=1, options=item_id, name=\"item\")\n\ndef get_preview(i):\n    return pn.panel(items[i].assets[\"rendered_preview\"].href, height=300)\n\n\npn.Row(item_sel, pn.bind(get_preview, item_sel))\n\nselected_item = items[1]\nselected_item\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#preview-results-and-select-a-dataset","position":13},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Access the Data","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#access-the-data","position":14},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Access the Data","lvl2":"Search for Landsat Data"},"content":"\n\nNow that we have selected a dataset from our catalog, we can procede to access the data. We want to be very selective about the data that we read and when we read it because the amount of downloaded data can quickly get out of hand. Therefore, let’s select only a subset of images.\n\nFirst, we’ll preview the different image assets (or \n\nBands) available in the Landsat item.\n\nassets = []\nfor _, asset in selected_item.assets.items():\n    try:\n        assets.append(asset.extra_fields[\"eo:bands\"][0])\n    except:\n        pass\n\ncols_ordered = [\n    \"common_name\",\n    \"description\",\n    \"name\",\n    \"center_wavelength\",\n    \"full_width_half_max\",\n]\nbands = pd.DataFrame.from_dict(assets)[cols_ordered]\nbands\n\nThen we will select a few bands (images) of interest:\n\nbands_of_interest = [\"red\", \"green\", \"blue\"]\n\nFinally, we lazily load the selected data. We will use the package called odc which allows us to load only a specific region of interest (bounding box or ‘bbox’) and specific bands (images) of interest. We will also use the chunks argument to load the data as dask arrays; this will load the metadata now and delay the loading until we actually use the data, or until we force the data to be loaded by using .compute().\n\nds = odc.stac.stac_load(\n    [selected_item],\n    bands=bands_of_interest,\n    bbox=bbox,\n    chunks={},  # <-- use Dask\n).isel(time=0)\nds\n\nLet’s combine the bands of the dataset into a single DataArray that has the band names as coordinates of a new ‘band’ dimension, and also call .compute() to finally load the data.\n\nda = ds.to_array(dim=\"band\").compute()\nda\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#access-the-data","position":15},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Visualize the data","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#visualize-the-data","position":16},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Visualize the data","lvl2":"Search for Landsat Data"},"content":"\n\nOften, data ingestion involves quickly visualizing your raw data to get a sense that things are proceeding accordingly. As we have created an array with red, blue, and green bands, we can quickly display a natural color image of the lake using the .plot.imshow() function of xarray. We’ll use the robust=True argument because the data values are outside the range of typical RGB images.\n\nda.plot.imshow(robust=True, size=3)\n\nNow, let’s use hvplot to provide an interactive visualization of the inividual bands in our array.\n\nds\n\nda.hvplot.image(x=\"x\", y=\"y\", cmap=\"viridis\", aspect=1)\n\nLet’s plot the bands as seperate columns by specifying a dimension to expand with col='band'. We can also set rasterize=True to use \n\nDatashader (another HoloViz tool) to render large data into a 2D histogram, where every array cell counts the data points falling into that pixel, as set by the resolution of your screen. This is especially important for large and high resolution images that would otherwise cause issues when attempting to render in a browser.\n\nda.hvplot.image(\n    x=\"x\", y=\"y\", col=\"band\", cmap=\"viridis\", xaxis=False, yaxis=False, colorbar=False, rasterize=True\n)\n\nSelect the zoom tool and zoom in on of the plots to see that all the images are all automatically linked!\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#visualize-the-data","position":17},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Retain Attributes","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#retain-attributes","position":18},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Retain Attributes","lvl2":"Search for Landsat Data"},"content":"\n\nWhen working with many image arrays, it’s critical to retain the data properties as xarray attributes:\n\nda.attrs = selected_item.properties\nda\n\nNotice that you can now expand the Attributes:  dropdown to see the properties of this data.\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#retain-attributes","position":19},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Set the crs attribute","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#set-the-crs-attribute","position":20},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Set the crs attribute","lvl2":"Search for Landsat Data"},"content":"\n\nAs the data is in ‘meter’ units from a reference point, we can plot in commonly used longitude, latitude coordinates with .hvplot(geo=True) if our array has a valid coordinate reference system (CRS) attribute. This value is provided from Microsoft Planetary Computer as the proj:epsg property, so we just need to copy it to a new attribute crs so that hvPlot can automatically find it, without us having to further specify anything in our plotting code\n\nNote, this CRS is referenced by an EPSG code that can be accessed from the metadata of our selected catalog search result. We can see more about this dataset’s specific code at \n\nEPSG.io/32611. You can also read more about EPSG codes in general in this \n\nCoordinate Reference Systems: EPSG codes online book chapter.\n\nda.attrs[\"crs\"] = f\"epsg:{selected_item.properties['proj:epsg']}\"\nda.attrs[\"crs\"]\n\nNow we can use .hvplot(geo=True) to plot in longitude and latitude coordinates. Informing hvPlot that this is geographic data also allows us to overlay data on aligned geographic tiles using the tiles parameter.\n\nda.hvplot.image(\n    x=\"x\", y=\"y\", cmap=\"viridis\", geo=True, alpha=.9, tiles=\"ESRI\", xlabel=\"Longitude\", ylabel=\"Latitude\", colorbar=False, aspect=1,\n)\n\n\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#set-the-crs-attribute","position":21},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Summary","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#summary","position":22},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Summary","lvl2":"Search for Landsat Data"},"content":"The data access approach should adapt to features of the data and your intended analysis. As Landsat data is large and multidimensional, a good approach is to use \n\nMicrosoft Plantery Computer, \n\npystac-client, and \n\nodc-stac together for searching the metadata catalog and lazily loading specific data chunks. Once you have accessed data, visualize it with hvPlot to ensure that it matches your expectations.","type":"content","url":"/notebooks/data-ingestion-geospatial#summary","position":23},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Search for Landsat Data"},"type":"lvl4","url":"/notebooks/data-ingestion-geospatial#whats-next","position":24},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Search for Landsat Data"},"content":"Before we proceed to workflow examples, we can explore an alternate way of accessing data using generalized tooling.\n\n","type":"content","url":"/notebooks/data-ingestion-geospatial#whats-next","position":25},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Resources and References","lvl2":"Search for Landsat Data"},"type":"lvl3","url":"/notebooks/data-ingestion-geospatial#resources-and-references","position":26},{"hierarchy":{"lvl1":"Data Ingestion - Geospatial-Specific Tooling","lvl3":"Resources and References","lvl2":"Search for Landsat Data"},"content":"Authored by Demetris Roumis circa Jan, 2023\n\nGuidance for parts of this notebook was provided by Microsoft in \n\n‘Reading Data from the STAC API’\n\nThe image used in the banner is from an announcement about PySTAC from Azavea","type":"content","url":"/notebooks/data-ingestion-geospatial#resources-and-references","position":27},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling"},"type":"lvl1","url":"/notebooks/data-ingestion-general","position":0},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling"},"content":"\n\n\n\n","type":"content","url":"/notebooks/data-ingestion-general","position":1},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/data-ingestion-general#overview","position":2},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Overview"},"content":"If the specialized geospatial tools discussed in the previous notebook suit your needs, feel free to proceed to explore a workflow example, such as \n\nSpectral Clustering. However, if you’re seeking a tool that is adaptable across a wider range of data types and sources, welcome to this introduction to \n\nIntake V2, a general-purpose data ingestion and management library.\n\nIntake is a high-level library designed for data ingestion and management. While the \n\ngeospatial-specific tooling approach is optimized for satellite data, Intake offers a broader and potentially more flexible approach for multimodal data workflows, characterized by:\n\nUnified Interface: Abstracts the details of data sources, enabling users to interact with a consistent API regardless of the data’s underlying format.\n\nDynamic and Shareable Catalogs: Facilitates the creation and sharing of data catalogs that can be version-controlled, updated, and maintained.\n\nExtensible: Facilitates the addition of new data sources and formats through its plugin system.\n\nIn the following sections, we will guide you through an introduction to various Intake functionalities that simplify data access and enhance both modularity and reproducibility in geospatial workflows.\n\n","type":"content","url":"/notebooks/data-ingestion-general#overview","position":3},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/data-ingestion-general#prerequisites","position":4},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Landsat\n\nNecessary\n\nBackground\n\nData Ingestion - Geospatial-Specific Tooling\n\nHelpful\n\n\n\nPandas Cookbook\n\nHelpful\n\n\n\nxarray Cookbook\n\nNecessary\n\n\n\nIntake Quickstart\n\nHelpful\n\n\n\nIntake Cookbook\n\nNecessary\n\n\n\nTime to learn: 20 minutes\n\n\n\n","type":"content","url":"/notebooks/data-ingestion-general#prerequisites","position":5},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/data-ingestion-general#imports","position":6},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Imports"},"content":"\n\nimport intake\nimport planetary_computer\nfrom pprint import pprint\n\n# Viz\nimport hvplot.xarray\nimport panel as pn\n\npn.extension()\n\n","type":"content","url":"/notebooks/data-ingestion-general#imports","position":7},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Connecting to Data Sources"},"type":"lvl2","url":"/notebooks/data-ingestion-general#connecting-to-data-sources","position":8},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Connecting to Data Sources"},"content":"\n\nTo get started, we need to provide a STAC URL (or any other data source URL) to intake, and we can ask intake to recommend some suitable datatypes.\n\nurl = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\ndata_types = intake.readers.datatypes.recommend(url)\npprint(data_types)\n\n","type":"content","url":"/notebooks/data-ingestion-general#connecting-to-data-sources","position":9},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Selecting the Appropriate Data Type"},"type":"lvl2","url":"/notebooks/data-ingestion-general#selecting-the-appropriate-data-type","position":10},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Selecting the Appropriate Data Type"},"content":"After identifying the possible data types, we choose the one that best suits our needs. For handling STAC formatted JSON data from our URL, we will proceed with STACJSON.\n\ndata_type = intake.datatypes.STACJSON(url)\ndata_type\n\nThis object now represents the specific data type we will work with, allowing us to streamline subsequent data operations.\n\n","type":"content","url":"/notebooks/data-ingestion-general#selecting-the-appropriate-data-type","position":11},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Initializing Data Readers"},"type":"lvl2","url":"/notebooks/data-ingestion-general#initializing-data-readers","position":12},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Initializing Data Readers"},"content":"With the STACJSON data type specified, we explore available methods to read the data.\n\nreaders = data_type.possible_readers\npprint(readers)\n\nThis output presents us with options that can interpret the STACJSON data format effectively. The StacCatalogReader is probably the most suitable for our use case. We can use it to read the STAC catalog and explore the available contents.\n\n","type":"content","url":"/notebooks/data-ingestion-general#initializing-data-readers","position":13},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Reading the Catalog"},"type":"lvl2","url":"/notebooks/data-ingestion-general#reading-the-catalog","position":14},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Reading the Catalog"},"content":"Next, we can access the data catalog through our reader.\n\nreader = intake.catalogs.StacCatalogReader(\n    data_type, signer=planetary_computer.sign_inplace\n)\nreader\n\nThis reader is now configured to handle interactions with the data catalog.\n\n","type":"content","url":"/notebooks/data-ingestion-general#reading-the-catalog","position":15},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"List Catalog Contents"},"type":"lvl2","url":"/notebooks/data-ingestion-general#list-catalog-contents","position":16},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"List Catalog Contents"},"content":"Once the catalog is accessible, we read() it and then collect each dataset’s description to identify datasets of interest. For our purposes, we will just print the entries that include the word 'landsat'.\n\nstac_cat = reader.read()\n\ndescription = {}\nfor data_description in stac_cat.data.values():\n    data = data_description.kwargs[\"data\"]\n    description[data[\"id\"]] = data[\"description\"]\n\n# Print only keys that include the word 'landsat'\npprint([key for key in description.keys() if 'landsat' in key.lower()])\n\n","type":"content","url":"/notebooks/data-ingestion-general#list-catalog-contents","position":17},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Detailed Dataset Examination"},"type":"lvl2","url":"/notebooks/data-ingestion-general#detailed-dataset-examination","position":18},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Detailed Dataset Examination"},"content":"\n\nBy examining specific datasets more closely, we understand their content and relevance to our project goals. We can now print the description of the desired landsat IDs.\n\nprint(\"1:\", description[\"landsat-c2-l1\"])\nprint('-------------------------------\\n')\nprint(\"2:\", description[\"landsat-c2-l2\"])\n\n","type":"content","url":"/notebooks/data-ingestion-general#detailed-dataset-examination","position":19},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Selecting and Accessing Data"},"type":"lvl2","url":"/notebooks/data-ingestion-general#selecting-and-accessing-data","position":20},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Selecting and Accessing Data"},"content":"We want \"landsat-c2-l2\", so with a chosen dataset, we can now access it directly and view the metadata specific to this dataset - key details that are important for analysis and interpretation. Since the output is long, we’ll utilize the HoloViz Panel library to wrap the output in a scrollable element.\n\nlandsat_reader = stac_cat[\"landsat-c2-l2\"]\nlandsat_metadata = landsat_reader.read().metadata\n\n# View extensive metadata in scrollable block\njson_pane = pn.pane.JSON(landsat_metadata, name='Metadata', max_height=400, sizing_mode='stretch_width', depth=-1, theme='light')\nscrollable_output = pn.Column(json_pane, height=400, sizing_mode='stretch_width', scroll=True, styles={'background': 'lightgrey'})\nscrollable_output\n\n","type":"content","url":"/notebooks/data-ingestion-general#selecting-and-accessing-data","position":21},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Visual Preview"},"type":"lvl2","url":"/notebooks/data-ingestion-general#visual-preview","position":22},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Visual Preview"},"content":"To get a visual preview of the dataset, particularly to check its quality and relevance, we use the following commands:\n\nlandsat_reader[\"thumbnail\"].read()\n\n","type":"content","url":"/notebooks/data-ingestion-general#visual-preview","position":23},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Accessing Geospatial Data Items"},"type":"lvl2","url":"/notebooks/data-ingestion-general#accessing-geospatial-data-items","position":24},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Accessing Geospatial Data Items"},"content":"Once we have selected the appropriate dataset, the next step is to access the specific data items. These items typically represent individual data files or collections that are part of the dataset.\n\nThe following code retrieves a handle to the ‘geoparquet-items’ from the Landsat dataset, which are optimized for efficient geospatial operations and queries.\n\nlandsat_items = landsat_reader[\"geoparquet-items\"]\nlandsat_items\n\n","type":"content","url":"/notebooks/data-ingestion-general#accessing-geospatial-data-items","position":25},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Converting Data for Analysis"},"type":"lvl2","url":"/notebooks/data-ingestion-general#converting-data-for-analysis","position":26},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Converting Data for Analysis"},"content":"To facilitate analysis, the following code selects the last few entries (tail) of the dataset, converts them into a GeoDataFrame, and reads it back into a STAC catalog format. This format is particularly suited for geospatial data and necessary for compatibility with geospatial analysis tools and libraries like Geopandas.\n\ncat = landsat_items.tail(output_instance=\"geopandas:GeoDataFrame\").GeoDataFrameToSTACCatalog.read()\n\n","type":"content","url":"/notebooks/data-ingestion-general#converting-data-for-analysis","position":27},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Exploring Data Collections"},"type":"lvl2","url":"/notebooks/data-ingestion-general#exploring-data-collections","position":28},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Exploring Data Collections"},"content":"After conversion, we explore the structure of the data collection. Each “item” in this collection corresponds to a set of assets, providing a structured way to access multiple related data files. We’ll simply print the structure of the catalog to understand the available items and their organization.\n\ncat\n\n","type":"content","url":"/notebooks/data-ingestion-general#exploring-data-collections","position":29},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Accessing Sub-Collections"},"type":"lvl2","url":"/notebooks/data-ingestion-general#accessing-sub-collections","position":30},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Accessing Sub-Collections"},"content":"To dive deeper into the data, we access a specific sub-collection based on its key. This allows us to focus on a particular geographic area or time period. We’ll select the first item in the catalog for now.\n\nitem_key = list(cat.entries.keys())[0]\nsubcat = cat[item_key].read()\nsubcat\n\n","type":"content","url":"/notebooks/data-ingestion-general#accessing-sub-collections","position":31},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Reading Specific Data Bands"},"type":"lvl2","url":"/notebooks/data-ingestion-general#reading-specific-data-bands","position":32},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Reading Specific Data Bands"},"content":"For detailed analysis, especially in remote sensing, accessing specific spectral bands is crucial. Here, we read the red spectral band, which is often used in vegetation analysis and other remote sensing applications.\n\nsubcat.red.read()\n\n","type":"content","url":"/notebooks/data-ingestion-general#reading-specific-data-bands","position":33},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Preparing for Multiband Analysis"},"type":"lvl2","url":"/notebooks/data-ingestion-general#preparing-for-multiband-analysis","position":34},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Preparing for Multiband Analysis"},"content":"To analyze true color imagery, we need to stack multiple spectral bands. Here, we prepare for this by setting up a band-stacking operation. Note, re-signing might be necessary at this point.\n\ncatbands = cat[item_key].to_reader(reader=\"StackBands\", bands=[\"red\", \"green\", \"blue\"], signer=planetary_computer.sign_inplace)\n\n","type":"content","url":"/notebooks/data-ingestion-general#preparing-for-multiband-analysis","position":35},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Loading and Visualizing True Color Imagery"},"type":"lvl2","url":"/notebooks/data-ingestion-general#loading-and-visualizing-true-color-imagery","position":36},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Loading and Visualizing True Color Imagery"},"content":"After setting up the band-stacking, we read the multiband data and prepare it for visualization.\n\ndata = catbands.read(dim=\"band\")\ndata\n\n","type":"content","url":"/notebooks/data-ingestion-general#loading-and-visualizing-true-color-imagery","position":37},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Visualizing Data"},"type":"lvl2","url":"/notebooks/data-ingestion-general#visualizing-data","position":38},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Visualizing Data"},"content":"Finally, we visualize the true color imagery. This visualization helps in assessing the quality of the data and the appropriateness of the bands used.\n\ndata.plot.imshow(robust=True, figsize=(10, 10))\n\n","type":"content","url":"/notebooks/data-ingestion-general#visualizing-data","position":39},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/data-ingestion-general#summary","position":40},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Summary"},"content":"As earth science data becomes integrated with other types of data, a powerful approach is to utilize a general purpose set of tools, including Intake and Xarray. Once you have accessed data, visualize it with hvPlot to ensure that it matches your expectations.","type":"content","url":"/notebooks/data-ingestion-general#summary","position":41},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/data-ingestion-general#whats-next","position":42},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl3":"What’s next?","lvl2":"Summary"},"content":"Now that we know how to access the data, it’s time to proceed to analysis, where we will explore a some simple machine learning approaches.","type":"content","url":"/notebooks/data-ingestion-general#whats-next","position":43},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/data-ingestion-general#resources-and-references","position":44},{"hierarchy":{"lvl1":"Data Ingestion - General Purpose Tooling","lvl2":"Resources and references"},"content":"Authored by Demetris Roumis and Andrew Huang circa April, 2024, with guidance from \n\nMartin Durant.\n\nThe banner image is a mashup of a Landsat 8 image from NASA and the Intake logo.","type":"content","url":"/notebooks/data-ingestion-general#resources-and-references","position":45},{"hierarchy":{"lvl1":"Spectral Clustering"},"type":"lvl1","url":"/notebooks/spectral-clustering-pc","position":0},{"hierarchy":{"lvl1":"Spectral Clustering"},"content":"\n\n\n\n","type":"content","url":"/notebooks/spectral-clustering-pc","position":1},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#overview","position":2},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Overview"},"content":"\n\nThe current notebook will demonstrate a simplified machine learning approach to observe the change in a lake water’s extent across time. In order to identify the water, we can use spectral clustering to classify each grid cell into a category based on the similarity of the combined set of pixels across \n\nwavelength-bands in our image stacks.\n\nOur example approach uses a version of spectral clustering from \n\ndask_ml that is a scalable equivalent of what is available in \n\nscikit-learn. We will begin this approach with a single image stack and then conduct a direct comparison on the results from different time points.\n\nThis workflow uses data from Microsoft Planetary Computer but it can be adapted to work with any data ingestion approach from this cookbook.\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#overview","position":3},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#prerequisites","position":4},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nData Ingestion - Geospatial-Specific Tooling\n\nNecessary\n\n\n\nscikit-learn\n\nHelpful\n\nSpectral clustering\n\ndask_ml\n\nHelpful\n\nSpectral clustering at scale\n\nTime to learn: 20 minutes.\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#prerequisites","position":5},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#imports","position":6},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Imports"},"content":"\n\nimport numpy as np\nimport odc.stac\nimport pandas as pd\nimport planetary_computer\nimport pystac_client\nimport xarray as xr\nfrom dask.distributed import Client\nfrom pystac.extensions.eo import EOExtension as eo\nfrom dask_ml.cluster import SpectralClustering\nimport pyproj\n\n# Viz\nimport hvplot.xarray\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#imports","position":7},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Loading Data"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#loading-data","position":8},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Loading Data"},"content":"\n\nLet’s start by loading some Landsat data. These steps are covered in the \n\nData Ingestion - Planetary Computer prerequisite.\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#loading-data","position":9},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Search the catalog","lvl2":"Loading Data"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#search-the-catalog","position":10},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Search the catalog","lvl2":"Loading Data"},"content":"\n\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\nbbox = [-118.89, 38.54, -118.57, 38.84]  # Region over a lake in Nevada, USA\ndatetime = \"2017-06-01/2017-09-30\"  # Summer months of 2017\ncollection = \"landsat-c2-l2\"\nplatform = \"landsat-8\"\ncloudy_less_than = 1  # percent\n\nsearch = catalog.search(\n    collections=[\"landsat-c2-l2\"],\n    bbox=bbox,\n    datetime=datetime,\n    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}, \"platform\": {\"in\": [platform]}},\n)\nitems = search.get_all_items()\nprint(f\"Returned {len(items)} Items:\")\n[[i, item.id] for i, item in enumerate(items)]\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#search-the-catalog","position":11},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Load a dataset","lvl2":"Loading Data"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#load-a-dataset","position":12},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Load a dataset","lvl2":"Loading Data"},"content":"\n\nitem = items[1]  # select one of the results\n\nassets = []\nfor _, asset in item.assets.items():\n    try:\n        assets.append(asset.extra_fields[\"eo:bands\"][0])\n    except:\n        pass\n\ncols_ordered = [\n    \"common_name\",\n    \"description\",\n    \"name\",\n    \"center_wavelength\",\n    \"full_width_half_max\",\n]\nbands = pd.DataFrame.from_dict(assets)[cols_ordered]\nbands\n\nds_2017 = odc.stac.stac_load(\n    [item],\n    bands=bands.common_name.values,\n    bbox=bbox,\n    chunks={},  # <-- use Dask\n).isel(time=0)\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#load-a-dataset","position":13},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Retain CRS Attribute","lvl2":"Loading Data"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#retain-crs-attribute","position":14},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Retain CRS Attribute","lvl2":"Loading Data"},"content":"\n\nepsg = item.properties[\"proj:epsg\"]\nds_2017.attrs[\"crs\"] = f\"epsg:{epsg}\"\n\nda_2017 = ds_2017.to_array(dim=\"band\")\nda_2017\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#retain-crs-attribute","position":15},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Reshaping Data"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#reshaping-data","position":16},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Reshaping Data"},"content":"The shape of our data is currently n_bands, n_y, n_x. In order for dask-ml / scikit-learn to consume our data, we’ll need to reshape our image stacks into n_samples, n_features, where n_features is the number of wavelength-bands and n_samples is the total number of pixels in each wavelength-band image. Essentially, we’ll be creating a vector of pixels out of each image, where each pixel has multiple features (bands), but the ordering of the pixels is no longer relevant to the computation.\n\nBy using xarray methods to flatten the data, we can keep track of the coordinate labels ‘x’ and ‘y’ along the way. This means that we have the ability to reshape back to our original array at any time with no information loss!\n\nflattened_xda = da_2017.stack(z=(\"x\", \"y\"))  # flatten each band\nflattened_t_xda = flattened_xda.transpose(\"z\", \"band\")\nflattened_t_xda\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#reshaping-data","position":17},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Standardize Data"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#standardize-data","position":18},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Standardize Data"},"content":"Now that we have the data in the correct shape, let’s standardize (or rescale) the values of the data. We do this to get all the flattened image vectors onto a common scale while preserving the differences in the ranges of values. Again, we’ll demonstrate doing this first in NumPy and then xarray.\n\nwith xr.set_options(keep_attrs=True):\n    rescaled_xda = (flattened_t_xda - flattened_t_xda.mean()) / flattened_t_xda.std()\nrescaled_xda\n\nInfoAbove, we are using a context manager \"with xr.set_options(keep_attrs=True):\" to retain the array's attributes through the operations. That is, we want any metadata like 'crs' to stay with our result so we can use 'geo=True' in our plotting.\n\nAs rescaled_xda is still a Dask object, if we wanted to actually run the rescaling at this point (provided that all the data can fit into memory), we would use rescaled_xda.compute().\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#standardize-data","position":19},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"ML pipeline"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#ml-pipeline","position":20},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"ML pipeline"},"content":"Now that our data is in the proper shape and value range, we are ready to conduct spectral clustering. Here we will use a version of \n\nspectral clustering from dask_ml that is a scalable equivalent to operations from Scikit-learn that cluster pixels based on similarity (across all wavelength-bands, which makes it spectral clustering by spectra!)\n\nclient = Client(processes=False)\nclient\n\nNow we will compute and persist the rescaled data to feed into the ML pipeline. Notice that our X matrix below has the shape: n_samples, n_features as discussed earlier.\n\nX = client.persist(rescaled_xda)\nX.shape\n\nFirst we will set up the model with the number of clusters, and other options.\n\nclf = SpectralClustering(\n    n_clusters=4,\n    random_state=0,\n    gamma=None,\n    kmeans_params={\"init_max_iter\": 5},\n    persist_embedding=True,\n)\n\nThis next step is the slow part. We’ll fit the model to our matrix X. Depending on your setup, it could take seconds to minutes to run depending on the size of our data.\n\n%time clf.fit(X)\n\nLet’s check the shape of the result:\n\nlabels = clf.assign_labels_.labels_.compute()\nlabels.shape\n\nlabels\n\nThe result is a single vector of cluster labels.\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#ml-pipeline","position":21},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Un-flattening"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#un-flattening","position":22},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Un-flattening"},"content":"Once the computation is done, we can use the coordinates of our input array to restack our output array back into an image. Again, one of the main benefits of using xarray for this stacking and unstacking is that it keeps track of the coordinate information for us.\n\nSince the original array is n_samples by n_features (90000, 6) and the cluster label output is (90000,), we just need the coordinates from one of the original features in the shape of n_samples. We can just copy the coordinates from the first input feature and populate is with our output data:\n\ntemplate = flattened_t_xda[:, 0]\noutput_array = template.copy(data=labels)\noutput_array\n\nWith this new output array with coordinates copied from the input array, we can unstack back to the original x and y image dimensions by just using .unstack().\n\nunstacked_2017 = output_array.unstack()\nunstacked_2017\n\nFinally, we can visualize the results! By hovering over the resulting imge, we can see that the lake water has been clustered with a certain label or ‘value’.\n\nraw_plot_2017 = da_2017.sel(band=\"red\").hvplot.image(\n    x=\"x\", y=\"y\", geo=True, xlabel=\"lon\", ylabel=\"lat\", datashade=True, cmap=\"greys\", title=\"Raw Image 2017\",\n)\n\nresult_plot_2017 = unstacked_2017.hvplot(\n    x=\"x\", y=\"y\", cmap=\"Set3\", geo=True, xlabel=\"lon\", ylabel=\"lat\", colorbar=False,  title=\"Spectral Clustering 2017\",\n)\n\nraw_plot_2017 + result_plot_2017\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#un-flattening","position":23},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Spectral Clustering for 1988"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#spectral-clustering-for-1988","position":24},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Spectral Clustering for 1988"},"content":"\n\nWe have conducted the spectral clustering for 2017 and now we want to compare this result to the lake in 1988. Let’s load data from 1988 and run the same analysis as above.\n\nWe will use the same catalog, but we will search it for a different point in time and different Landsat mission\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#spectral-clustering-for-1988","position":25},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Load the data","lvl2":"Spectral Clustering for 1988"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#load-the-data","position":26},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Load the data","lvl2":"Spectral Clustering for 1988"},"content":"\n\nbbox = [-118.89, 38.54, -118.57, 38.84]  # Region over a lake in Nevada, USA\ndatetime = \"1988-06-01/1988-09-30\"  # Summer months of 1988\ncollection = \"landsat-c2-l2\"\nplatform = \"landsat-5\"  # Searching through an earlier landsat mission\ncloudy_less_than = 1  # percent\n\nsearch = catalog.search(\n    collections=[\"landsat-c2-l2\"],\n    bbox=bbox,\n    datetime=datetime,\n    query={\"eo:cloud_cover\": {\"lt\": cloudy_less_than}, \"platform\": {\"in\": [platform]}},\n)\n\nitems = search.get_all_items()\nitem = items[1]  # select one of the results\n\nNotice that Landsat 5 data from 1988 has slightly different spectra than Landsat 8 from 2017. Details like this are important to keep in mind when performing analyses that directly compare across missions.\n\nassets = []\nfor _, asset in item.assets.items():\n    try:\n        assets.append(asset.extra_fields[\"eo:bands\"][0])\n    except:\n        pass\n\ncols_ordered = [\n    \"common_name\",\n    \"description\",\n    \"name\",\n    \"center_wavelength\",\n    \"full_width_half_max\",\n]\nbands = pd.DataFrame.from_dict(assets)[cols_ordered]\nbands\n\nds_1988 = odc.stac.stac_load(\n    [item],\n    bands=bands.common_name.values,\n    bbox=bbox,\n    chunks={},  # <-- use Dask\n).isel(time=0)\n\nepsg = item.properties[\"proj:epsg\"]\nds_1988.attrs[\"crs\"] = f\"epsg:{epsg}\"\n\nda_1988 = ds_1988.to_array(dim=\"band\")\nda_1988\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#load-the-data","position":27},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Reshape and Standardize","lvl2":"Spectral Clustering for 1988"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#reshape-and-standardize","position":28},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Reshape and Standardize","lvl2":"Spectral Clustering for 1988"},"content":"\n\nflattened_xda = da_1988.stack(z=(\"x\", \"y\"))\nflattened_t_xda = flattened_xda.transpose(\"z\", \"band\")\nwith xr.set_options(keep_attrs=True):\n    rescaled_xda = (flattened_t_xda - flattened_t_xda.mean()) / flattened_t_xda.std()\nrescaled_xda\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#reshape-and-standardize","position":29},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Spectral Clustering","lvl2":"Spectral Clustering for 1988"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#spectral-clustering","position":30},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Spectral Clustering","lvl2":"Spectral Clustering for 1988"},"content":"\n\nX = client.persist(rescaled_xda)\nclf = SpectralClustering(\n    n_clusters=4,\n    random_state=0,\n    gamma=None,\n    kmeans_params={\"init_max_iter\": 5},\n    persist_embedding=True,\n)\n\n%time clf.fit(X)\n\nlabels = clf.assign_labels_.labels_.compute()\nlabels.shape\n\nlabels\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#spectral-clustering","position":31},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Unstack and Visualize","lvl2":"Spectral Clustering for 1988"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#unstack-and-visualize","position":32},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"Unstack and Visualize","lvl2":"Spectral Clustering for 1988"},"content":"\n\ntemplate = flattened_t_xda[:, 0]\noutput_array = template.copy(data=labels)\nunstacked_1988 = output_array.unstack()\n\nunstacked_1988\n\nraw_plot_1988 = da_1988.sel(band=\"red\").hvplot.image(\n    x=\"x\", y=\"y\", geo=True, xlabel=\"lon\", ylabel=\"lat\", datashade=True, cmap=\"greys\", title=\"Raw 1988\"\n)\n\nresult_plot_1988 = unstacked_1988.hvplot(\n    x=\"x\", y=\"y\", cmap=\"Set3\", geo=True, xlabel=\"lon\", ylabel=\"lat\", colorbar=False, title=\"Spectral Clustering 1988\",\n)\n\nraw_plot_1988 + result_plot_1988\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#unstack-and-visualize","position":33},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Spectral Clustering Over Time"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#spectral-clustering-over-time","position":34},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Spectral Clustering Over Time"},"content":"\n\nOur hypothesis is that the lake’s area is receding over time and so we want to visualize the potential change. Let’s first visually compare the plot of the clustering results from the different time points.\n\nresult_plot_1988 + result_plot_2017\n\nBy hovering over the lake in each image, we can see that the water was labeled (‘value’) with a certain cluster number in both images. We will programmatically grab the water cluster value from the middle of the lake using pyproj to convert from longtitude/latitude coordinates.\n\nlon_lake_center = -118.71\nlat_lake_center = 38.7\n\nproj = pyproj.Proj(unstacked_1988.crs)\nlake_center_x, lake_center_y = proj(lon_lake_center, lat_lake_center)\n\nwater_cluster_1988 = int(unstacked_1988.sel(x=lake_center_x, y=lake_center_y, method='nearest'))\nwater_cluster_2017 = int(unstacked_2017.sel(x=lake_center_x, y=lake_center_y, method='nearest'))\n\nprint('water cluster values:', water_cluster_1988, water_cluster_2017)\n\nNow, let’s set any value that isn’t our water cluster label to 0.\n\nwith xr.set_options(keep_attrs=True):\n    water_1988 = (unstacked_1988 == water_cluster_1988).astype(int)\n    water_2017 = (unstacked_2017 == water_cluster_2017).astype(int)\n\n\nwater_1988_plot = water_1988.hvplot(\n    x=\"x\", y=\"y\", cmap=\"greys\", geo=True, colorbar=False, title=\"1988 Water\"\n)\n\nwater_2017_plot = water_2017.hvplot(\n    x=\"x\", y=\"y\", cmap=\"greys\", geo=True, colorbar=False, title=\"2017 Water\"\n)\n\nwater_1988_plot + water_2017_plot\n\nNow we can take the difference of these water label arrays to see exactly where the water levels has changed.\n\nwith xr.set_options(keep_attrs=True):\n    water_diff = water_1988 - water_2017\n\nRed pixels (array value ‘1’) of our image below are where water was lost from 1988 to 2017.\n\nwater_diff.hvplot(\n    x=\"x\", y=\"y\", cmap='coolwarm', geo=True, xlabel=\"long\", ylabel=\"lat\", colorbar=False, title=\"Water Change 1988-2017\",\n)\n\nWe did it! We are observing the change in the lake shoreline over time using a simple spectral clustering approach.\n\nLet’s finish things off by adding some geo tiles as a background. To only display the colored pixels overlaid on geo tiles, we could either set the array’s background value (‘0’) to ‘Not a Number’ (NaN), or we could just inform hvPlot that we want the background valued pixels to be transparent with .redim.nodata(value=0).\n\nwater_diff.hvplot(\n        x=\"x\", y=\"y\", width=400, height=400, cmap='coolwarm', geo=True, xlabel=\"lon\", ylabel=\"lat\", alpha=1, colorbar=False, title=\"Water Loss from 1988 to 2017\", tiles=\"ESRI\",\n).redim.nodata(value=0)\n\n\n\n\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#spectral-clustering-over-time","position":35},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#summary","position":36},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Summary"},"content":"Starting from raw Landsat data, we have used a simple spectral clustering approach to observe the change in a lake water’s extent across time.","type":"content","url":"/notebooks/spectral-clustering-pc#summary","position":37},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/spectral-clustering-pc#whats-next","position":38},{"hierarchy":{"lvl1":"Spectral Clustering","lvl3":"What’s next?","lvl2":"Summary"},"content":"Adapt this notebook for your own use case or select another workflow example notebook.\n\n","type":"content","url":"/notebooks/spectral-clustering-pc#whats-next","position":39},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Resources and References"},"type":"lvl2","url":"/notebooks/spectral-clustering-pc#resources-and-references","position":40},{"hierarchy":{"lvl1":"Spectral Clustering","lvl2":"Resources and References"},"content":"Authored by Demetris Roumis circa Jan, 2023","type":"content","url":"/notebooks/spectral-clustering-pc#resources-and-references","position":41}]}